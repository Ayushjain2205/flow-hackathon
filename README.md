# FLOW.ai

## The problem Flow.ai solves

How might we get AI from a generative phase to an AGENTIC phase?

We bring it here for FLOW. We have a FLOW anchor ⚓

Imagine the target audience of FLOW comes to the platform, go to the “Flow agent” section. The agent perform all tasks (with some help from the user) , like :

- Send money 🤑
- Buy a NFT 🤩
- Set reminders to send money, keep an eye on NFT drops and more 😉
- Know completely about the wallet health 😰
- View the upcoming drop calendar 🥳
- & more more 🥶

But wait, it doesn’t end here, We also have DEV MODE. Yes, we want to cater to all the target audience of FLOW.

In Dev mode, users are able to :

- Deploy smart contract 😳
- Debug your code 😅
- Receive all updates about FLOW 😚
- Stay connected to the FLOW community and more.

But what the most attractive aspect of the product, and what our team has also attempted to work hard at, is to make it super easy for the user to understand and navigate.

We have this AI revolution right now but what really makes it useful is how easy it is for the user to interact with these agents. We want to make it exciting for them. It should feel like more than a chat, right?

When a user is interacting with an AGENT, the user should feel they are interacting with something that is more intelligent than a chat bot. The user should trust the agent, know what the agent can do. The visual cues about the capabilities of the agent become very important because that is the only way users can navigate through the capabilities. We’ve incorporated such aspects in different ways, and will be easily visible in the demo.

The most crucial part of the process was to understand the FLOW ecosystem from start to end in order to embody it completely for the user. We assessed the capabilities which should be implemented fro the MVP. We build upwards from here.

The process of creating and implementing a design thinking process this has been so energetic.
We are so excited. Let’s do this!

## Challenges we ran into

- UX POV : To visualise this product was tough from the beginning. The interaction implemented for chat does work for templates but was not feasible for chat because the user doesn’t have a designated text box. When a user is interacting with an agent, the capability depiction should be done very clearly. In our first iteration the user couldn’t understand what to ask the agent because they didn't know what it can do, which made us introduce the section ‘KNOW ABOUT AGENT”. Numerous aspects as such were to be carefully thought of and implemented.

- TECH POV: Leveraging an LLM to comprehend user intent contextually and subsequently rendering the appropriate template and executing the necessary API calls was a challenging task. We encountered difficulties in finding APIs that could provide real-time NFT information, so we had to develop our own web scrapers and categorize the data into various sources
